# -*- coding: utf-8 -*-
"""Tugas NLP 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cyCr2Ee7oBmWiwbYd4JKKGOUezP46bT-

# Biodata

**Fida Mardliyah // fidamardliyah11@gmail.com**

Sumber Data : https://www.kaggle.com/ishantjuyal/language-detection-dataset

# Import Data
"""

import pandas as pd
import numpy as np

data = pd.read_csv('ireland-news-headlines.csv')

data.head()

data.info()

data.shape

"""# Preprocessing

## Missing Value
"""

data.isnull().values.any()

data = data.drop(columns='publish_date')

data

"""## Label Desc"""

import matplotlib.pyplot as plt
import seaborn as sns

sns.countplot(x='headline_category', data=data)

"""## One-Hot Encodding"""

category =pd.get_dummies(data.headline_category)
datanew = pd.concat([data, category], axis=1)
datanew = datanew.drop(columns='headline_category')
datanew

text = datanew['Text_Snippet'].values
label = datanew[['Books', 'Film', 'Music']].values

"""# Split Data"""

from sklearn.model_selection import train_test_split
text_train, text_val, label_train, label_val = train_test_split(text, label, test_size=0.2)

"""# Tokenizer & Sequences"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=8000, oov_token='<OOV>')
tokenizer.fit_on_texts(text_train)
tokenizer.fit_on_texts(text_val)

sequences_train = tokenizer.texts_to_sequences(text_train)
sequences_val = tokenizer.texts_to_sequences(text_val)

padded_train = pad_sequences(sequences_train) 
padded_val = pad_sequences(sequences_val)

"""# Modelling"""

import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from tensorflow.keras.layers import Activation
from keras.layers.embeddings import Embedding
from tensorflow.keras.layers import Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

"""## Sequential"""

model = Sequential([
    Embedding(input_dim=8000, output_dim=128),
    LSTM(128),
    Dense(256, activation='relu'),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(3, activation='softmax')
])

model.summary()

"""## Compile Optimizer"""

model.compile(
    loss='categorical_crossentropy',
    optimizer=Adam(),
    metrics=['accuracy']
)

"""## Callbacks"""

class mycb(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs=None):
    if(logs.get('accuracy') >= 0.95 and logs.get('val_accuracy') >= 0.90):
      print('\nFor Epoch', epoch, '\nAccuracy has reach = %2.2f%%' %(logs['accuracy']*100), 'stop training.'),
      self.model.stop_training = True

"""# Fit Training"""

fitmodel = model.fit(padded_train, label_train,
                     steps_per_epoch = 25, epochs = 100, 
                     validation_data = (padded_val, label_val), validation_steps=5,
                     verbose=2, callbacks = [mycb()]
                     )

"""# Perbandingan Plot

## Akurasi
"""

epochs = range(len(fitmodel.history['accuracy']))

plt.plot(epochs, fitmodel.history['accuracy'], label='Training Accuracy', color='blue')
plt.plot(fitmodel.history['val_accuracy'], label='Validation Accuracy', color='green')
plt.title('Perbandingan Ukuran Akurasi')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc=0)
plt.figure()

plt.show()

"""## Loss"""

plt.plot(epochs, fitmodel.history['loss'], label='Training Loss', color='blue')
plt.plot(epochs, fitmodel.history['val_loss'], label='Validation Loss', color = 'green')
plt.title('Loss Training & Validation')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc=0)
plt.figure()

plt.show()